{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf8671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import warnings\n",
    "import scipy.sparse\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# DL Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D , MaxPooling2D ,Dropout , Flatten , Dense ,BatchNormalization ,Concatenate ,Input \n",
    "from keras.models import Sequential ,Model\n",
    "import json\n",
    "\n",
    "# other libraries\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import string\n",
    "import re\n",
    "from diptest import diptest \n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, TruncatedSVD  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.ensemble import RandomForestClassifier # Added RandomForestClassifier\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score # Changed metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Import classification metrics\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, LabelEncoder\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import iqr # For Silverman's rule\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# RLAC MODEL\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks  \n",
    "from sklearn.neighbors import KernelDensity  \n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from scipy import stats\n",
    "from scipy.special import eval_hermitenorm  # For normalized Hermite polynomials H_n(x)\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import (adjusted_mutual_info_score, adjusted_rand_score, \n",
    "                             homogeneity_score, completeness_score, v_measure_score,\n",
    "                             fowlkes_mallows_score, silhouette_score, calinski_harabasz_score,\n",
    "                             davies_bouldin_score)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef850ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes: [('x', 'REAL'), ('y', 'REAL'), ('class', ['1', '2', '3', '4', '5', '6', '7'])]\n",
      "Data shape: (788, 3)\n"
     ]
    }
   ],
   "source": [
    "import arff \n",
    "\n",
    "# Load the ARFF file\n",
    "dataset = arff.load(open(r'aggregation.arff'))\n",
    "\n",
    "# Access the data and attributes\n",
    "data = np.array(dataset['data'])\n",
    "attributes = dataset['attributes']\n",
    "\n",
    "# Print some information (optional)\n",
    "print(\"Attributes:\", attributes)\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ca7e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x      y class\n",
      "0  15.55  28.65     2\n",
      "1   14.9  27.55     2\n",
      "2  14.45  28.35     2\n",
      "3  14.15   28.8     2\n",
      "4  13.75  28.05     2\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=[attr[0] for attr in attributes])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564950b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and labels (y)\n",
    "X = df[['x', 'y']].values  # Features (x and y coordinates)\n",
    "y_train = df['class'].values      # Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc2c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e754e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9ee57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering  \n",
    "from sklearn.cluster import SpectralClustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fbd2230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Apply clustering algorithms\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#kmeans = KMeans(n_clusters=7, n_init= 10)  \n",
    "kmeans = KMeans(n_clusters=7, n_init=10, init='random', algorithm='lloyd', max_iter=10, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_train)\n",
    "\n",
    "ami_km = adjusted_mutual_info_score(y_train, kmeans_labels)\n",
    "ari_km = adjusted_rand_score(y_train, kmeans_labels)\n",
    "\n",
    "results.append([\"KMeans\", ami_km, ari_km])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "113827d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncut = SpectralClustering(n_clusters=7, affinity='nearest_neighbors', assign_labels='kmeans', n_jobs=-1)  \n",
    "ncut_labels = ncut.fit_predict(X_train)\n",
    "\n",
    "ami_nc = adjusted_mutual_info_score(y_train, ncut_labels)\n",
    "ari_nc = adjusted_rand_score(y_train, ncut_labels)\n",
    "\n",
    "results.append([\"NClust\", ami_nc, ari_nc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f302d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hclust = AgglomerativeClustering(n_clusters=7, linkage='ward') \n",
    "hclust = AgglomerativeClustering(n_clusters=7, linkage='single', metric='euclidean')\n",
    "hclust_labels = hclust.fit_predict(X_train)\n",
    "\n",
    "ami_hc = adjusted_mutual_info_score(y_train, hclust_labels)\n",
    "ari_hc = adjusted_rand_score(y_train, hclust_labels)\n",
    "\n",
    "results.append([\"HClust(single)\", ami_hc, ari_hc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab258fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model       AMI       ARI\n",
      "0          KMeans  0.837045  0.737531\n",
      "1          NClust  0.781503  0.543884\n",
      "2  HClust(single)  0.883347  0.805773\n"
     ]
    }
   ],
   "source": [
    "# Create Pandas DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"AMI\", \"ARI\"])\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab0753fd-46ae-4886-ac95-61d5a4d336ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Get the path of the parent directory (Project_Root)\n",
    "# '..' means \"go up one level\"\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# 2. Add it to Python's search path if not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# 3. Now you can import normally\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "#print(\"Successfully imported models from:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "175c0ef9-f97d-43e9-b375-6421f2351aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Benchmark on Aggregation Dataset (n=788, k=7)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Running RLAC depth_ratio     | r=50, bw=0.3, s=44 ... [DEPTH_RATIO] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 0 | Score: 0.8761\n",
      "Iter 2: Split Cluster 0 (Size: 232) via Proj 14 | Score: 0.7453\n",
      "Iter 3: Split Cluster 0 (Size: 556) via Proj 20 | Score: 0.7132\n",
      "Iter 4: Split Cluster 2 (Size: 215) via Proj 0 | Score: 0.3899\n",
      "Iter 5: Split Cluster 2 (Size: 341) via Proj 31 | Score: 0.2980\n",
      "Iter 6: Split Cluster 4 (Size: 307) via Proj 6 | Score: 0.2682\n",
      "RLAC (depth_ratio) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC dip             | r=50, bw=0.3, s=44 ... [DIP] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 3 | Score: 0.0612\n",
      "Iter 2: Split Cluster 1 (Size: 232) via Proj 20 | Score: 0.0507\n",
      "Iter 3: Split Cluster 0 (Size: 556) via Proj 14 | Score: 0.0495\n",
      "Iter 4: Split Cluster 3 (Size: 215) via Proj 3 | Score: 0.0370\n",
      "Iter 5: Split Cluster 2 (Size: 341) via Proj 31 | Score: 0.0173\n",
      "Iter 6: Split Cluster 4 (Size: 307) via Proj 26 | Score: 0.0172\n",
      "RLAC (dip) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC holes           | r=50, bw=0.3, s=44 ... [HOLES] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "RLAC (holes) complete. Final clusters: 1\n",
      "Done (AMI: 0.0000)\n",
      "\n",
      "Running RLAC min_kurt        | r=50, bw=0.3, s=44 ... [MIN_KURT] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 14 | Score: 1.4235\n",
      "Iter 2: Split Cluster 1 (Size: 344) via Proj 0 | Score: 1.6456\n",
      "Iter 3: Split Cluster 0 (Size: 444) via Proj 0 | Score: 0.8669\n",
      "Iter 4: Split Cluster 1 (Size: 215) via Proj 0 | Score: 0.4358\n",
      "Iter 5: Split Cluster 2 (Size: 341) via Proj 6 | Score: 0.3543\n",
      "Iter 6: Split Cluster 4 (Size: 306) via Proj 0 | Score: 0.3872\n",
      "RLAC (min_kurt) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC max_kurt        | r=50, bw=0.3, s=44 ... [MAX_KURT] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 31 | Score: -0.7164\n",
      "Iter 2: Split Cluster 0 (Size: 754) via Proj 0 | Score: -1.2992\n",
      "Iter 3: Split Cluster 1 (Size: 232) via Proj 14 | Score: -1.3836\n",
      "Iter 4: Split Cluster 1 (Size: 522) via Proj 14 | Score: -1.3937\n",
      "Iter 5: Split Cluster 3 (Size: 307) via Proj 6 | Score: -0.2702\n",
      "Iter 6: Split Cluster 3 (Size: 215) via Proj 0 | Score: -0.4358\n",
      "RLAC (max_kurt) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC negentropy      | r=50, bw=0.3, s=44 ... [NEGENTROPY] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 3 | Score: 0.3641\n",
      "Iter 2: Split Cluster 0 (Size: 556) via Proj 14 | Score: 0.3956\n",
      "Iter 3: Split Cluster 2 (Size: 215) via Proj 3 | Score: 0.3249\n",
      "Iter 4: Split Cluster 0 (Size: 232) via Proj 20 | Score: 0.2789\n",
      "Iter 5: Split Cluster 0 (Size: 341) via Proj 31 | Score: 0.1392\n",
      "Iter 6: Split Cluster 4 (Size: 307) via Proj 6 | Score: 0.1422\n",
      "RLAC (negentropy) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC skewness        | r=50, bw=0.3, s=44 ... [SKEWNESS] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 31 | Score: 0.2720\n",
      "Iter 2: Split Cluster 0 (Size: 754) via Proj 0 | Score: 0.2042\n",
      "Iter 3: Split Cluster 2 (Size: 522) via Proj 14 | Score: 0.2944\n",
      "Iter 4: Split Cluster 3 (Size: 215) via Proj 0 | Score: 0.8007\n",
      "Iter 5: Split Cluster 2 (Size: 307) via Proj 6 | Score: 0.5805\n",
      "Iter 6: Split Cluster 1 (Size: 232) via Proj 14 | Score: 0.1148\n",
      "RLAC (skewness) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC fisher          | r=50, bw=0.3, s=44 ... [FISHER] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 14 | Score: 11.3524\n",
      "Iter 2: Split Cluster 1 (Size: 344) via Proj 0 | Score: 12.2000\n",
      "Iter 3: Split Cluster 2 (Size: 215) via Proj 0 | Score: 12.8909\n",
      "Iter 4: Split Cluster 0 (Size: 444) via Proj 0 | Score: 9.7492\n",
      "Iter 5: Split Cluster 4 (Size: 341) via Proj 31 | Score: 8.4384\n",
      "Iter 6: Split Cluster 4 (Size: 307) via Proj 6 | Score: 8.3392\n",
      "RLAC (fisher) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC hermite         | r=50, bw=0.3, s=44 ... [HERMITE] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 14 | Score: 2.0544\n",
      "Iter 2: Split Cluster 1 (Size: 344) via Proj 0 | Score: 2.6776\n",
      "Iter 3: Split Cluster 0 (Size: 444) via Proj 0 | Score: 0.8829\n",
      "Iter 4: Split Cluster 1 (Size: 215) via Proj 0 | Score: 0.8253\n",
      "Iter 5: Split Cluster 2 (Size: 341) via Proj 6 | Score: 0.2926\n",
      "Iter 6: Split Cluster 4 (Size: 306) via Proj 31 | Score: 0.4082\n",
      "RLAC (hermite) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running RLAC friedman_tukey  | r=50, bw=0.3, s=44 ... [FRIEDMAN_TUKEY] Generating 50 sparse random projections...\n",
      "Starting Clustering: Target=7 clusters.\n",
      "Iter 1: Split Cluster 0 (Size: 788) via Proj 14 | Score: 117.7259\n",
      "Iter 2: Split Cluster 1 (Size: 344) via Proj 0 | Score: 74.4749\n",
      "Iter 3: Split Cluster 0 (Size: 444) via Proj 0 | Score: 60.6274\n",
      "Iter 4: Split Cluster 3 (Size: 341) via Proj 31 | Score: 48.8300\n",
      "Iter 5: Split Cluster 1 (Size: 215) via Proj 0 | Score: 44.4114\n",
      "Iter 6: Split Cluster 2 (Size: 307) via Proj 6 | Score: 41.4062\n",
      "RLAC (friedman_tukey) complete. Final clusters: 7\n",
      "Done (AMI: 0.9914)\n",
      "\n",
      "Running MDH Standard        | h=1.0, a=0.9 ... \n",
      "Done (AMI: 0.8290)\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS (ALL MODELS - SORTED BY AMI)\n",
      "================================================================================\n",
      "Model         Method             Params      AMI      ARI\n",
      " RLAC    depth_ratio r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC            dip r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC       min_kurt r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC         fisher r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC       max_kurt r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC     negentropy r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC       skewness r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC friedman_tukey r=50, bw=0.3, s=44 0.991408 0.994888\n",
      " RLAC        hermite r=50, bw=0.3, s=44 0.991408 0.994888\n",
      "  MDH       Standard              Fixed 0.829005 0.737624\n",
      " RLAC          holes r=50, bw=0.3, s=44 0.000000 0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# Import your custom models\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Target clusters for Aggregation dataset (should be 7)\n",
    "n_clusters = len(set(y_train))\n",
    "\n",
    "# RLAC Parameters\n",
    "rlac_methods = [\n",
    "    'depth_ratio', 'dip', 'holes', 'min_kurt', 'max_kurt', \n",
    "    'negentropy', 'skewness', 'fisher', 'hermite', 'friedman_tukey'\n",
    "]\n",
    "rlac_params = {\n",
    "    'random_state': [44],\n",
    "    'bw_adjust': [0.3],\n",
    "    'r': [50]\n",
    "}\n",
    "\n",
    "# MDH Parameters (Same as Anuran configuration)\n",
    "mdh_config = {\n",
    "    \"h_multiplier\": 1.0,\n",
    "    \"alphamax_val\": 0.9,\n",
    "    \"alpha_steps\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"\\nStarting Benchmark on Aggregation Dataset (n={len(X_train)}, k={n_clusters})...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 1. RLAC LOOP\n",
    "# ==========================================\n",
    "for method in rlac_methods:\n",
    "    for r_val in rlac_params['r']:\n",
    "        for bw in rlac_params['bw_adjust']:\n",
    "            for seed in rlac_params['random_state']:\n",
    "                \n",
    "                # 1. Print parameters BEFORE running\n",
    "                param_str = f\"r={r_val}, bw={bw}, s={seed}\"\n",
    "                print(f\"\\nRunning RLAC {method:<15} | {param_str} ... \", end=\"\")\n",
    "                \n",
    "                try:\n",
    "                    # 2. Instantiate & Fit\n",
    "                    model = RLAC(\n",
    "                        n_clusters=n_clusters,\n",
    "                        method=method,\n",
    "                        r=r_val,\n",
    "                        bw_adjust=bw,\n",
    "                        random_state=seed,\n",
    "                        plot=False\n",
    "                    )\n",
    "                    \n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                        model.fit(X_train)\n",
    "                    \n",
    "                    # 3. Evaluate\n",
    "                    ami = adjusted_mutual_info_score(y_train, model.labels_)\n",
    "                    ari = adjusted_rand_score(y_train, model.labels_)\n",
    "                    \n",
    "                    # 4. Log Success\n",
    "                    print(f\"Done (AMI: {ami:.4f})\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Model': 'RLAC',\n",
    "                        'Method': method,\n",
    "                        'Params': param_str,\n",
    "                        'AMI': ami,\n",
    "                        'ARI': ari\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"FAILED. Error: {e}\")\n",
    "                    results.append({\n",
    "                        'Model': 'RLAC', 'Method': method, 'Params': param_str,\n",
    "                        'AMI': -1, 'ARI': -1\n",
    "                    })\n",
    "\n",
    "# ==========================================\n",
    "# 2. MDH RUN\n",
    "# ==========================================\n",
    "print(f\"\\nRunning MDH {'Standard':<15} | h=1.0, a=0.9 ... \")\n",
    "try:\n",
    "    mdh_model = MDH(\n",
    "        n_clusters=n_clusters,\n",
    "        h_multiplier=mdh_config['h_multiplier'],\n",
    "        alphamax_val=mdh_config['alphamax_val'],\n",
    "        alpha_steps=mdh_config['alpha_steps'],\n",
    "        random_state=mdh_config['random_state'],\n",
    "        verbose=False,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    mdh_model.fit(X_train)\n",
    "    \n",
    "    ami_mdh = adjusted_mutual_info_score(y_train, mdh_model.labels_)\n",
    "    ari_mdh = adjusted_rand_score(y_train, mdh_model.labels_)\n",
    "    \n",
    "    print(f\"Done (AMI: {ami_mdh:.4f})\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'MDH',\n",
    "        'Method': 'Standard',\n",
    "        'Params': 'Fixed',\n",
    "        'AMI': ami_mdh,\n",
    "        'ARI': ari_mdh\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAILED. Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. RESULTS TABLE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS (ALL MODELS - SORTED BY AMI)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Create DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='AMI', ascending=False)\n",
    "\n",
    "# 2. Print FULL table (no truncation)\n",
    "# index=False hides the row numbers for a cleaner look\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb58967-dd00-43a6-8a56-a370eabdc763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9b88e-296a-4ec6-a44e-473f5ed26f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
